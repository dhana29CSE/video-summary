# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BzHn8VuytABmXN_OCVlA2_E0w0mVGhC-
"""

# ===============================
# ‚úÖ Video Summarizer v3 (Fixed)
# ===============================

# -------------------------------
# 0Ô∏è‚É£ Install & Imports
# -------------------------------
#!pip install -q ffmpeg-python transformers sentence-transformers git+https://github.com/openai/whisper.git
#!apt-get install -y ffmpeg
#!pip install -q scikit-learn nltk

def run_summarizer(input_video, language="original"):
    import os, json, numpy as np, torch, shutil
    import ffmpeg
    import whisper
    from sentence_transformers import SentenceTransformer, util
    from transformers import pipeline
    from nltk.tokenize import sent_tokenize
    import nltk

    nltk.download("punkt")

    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)

    asr_model = whisper.load_model("medium")
    embedding_model = SentenceTransformer("distiluse-base-multilingual-cased-v2")
    device = 0 if torch.cuda.is_available() else -1
    summarizer = pipeline("summarization", model="csebuetnlp/mT5_multilingual_XLSum", device=device)

    # -------------------------------
    # language selection (NO input())
    # -------------------------------
    translate = language == "english"
    task = "translate" if translate else "transcribe"

    # your remaining logic continues from here‚Ä¶
    # (do NOT hard-code input_video again, keep the parameter!)

    # -------------------------------
    # 2Ô∏è‚É£ Preprocess video (normalize audio)
    # -------------------------------
    def preprocess_video(input_path, output_path="preprocessed_video.mp4"):
        print(f"‚öôÔ∏è Preprocessing video: {input_path}")
        cmd = (
            f'ffmpeg -i "{input_path}" -af "loudnorm" -c:v libx264 -preset veryfast '
            f'-c:a aac -b:a 128k -y "{output_path}"'
        )
        os.system(cmd)
        return output_path

    preprocessed_path = preprocess_video(input_video)

    # -------------------------------
    # 3Ô∏è‚É£ Transcription
    # -------------------------------

    print("üîä Transcribing video...")
    result = asr_model.transcribe(preprocessed_path, task=task, word_timestamps=True)

    # Save transcript
    transcript_file = os.path.join(output_dir, "transcript.json")
    with open(transcript_file, "w") as f:
        json.dump(result['segments'], f, indent=2)
    print("Transcript saved at:", transcript_file)

    # =====================================================================
    # 4Ô∏è‚É£ ADVANCED Sentence Tokenization + SMART Timestamp Mapping
    # =====================================================================

    sentences, timestamps = [], []

    filler_words = set([
        "uh", "um", "you know", "like", "basically", "actually",
        "so", "well", "okay", "right", "literally"
    ])

    def clean_sentence(s):
        s = s.strip().replace("\n", " ")
        for f in filler_words:
            if s.lower().startswith(f):
                s = s[len(f):].strip()
        return s

    for seg in result["segments"]:
        seg_start, seg_end = seg["start"], seg["end"]
        seg_text = clean_sentence(seg["text"])

        raw_sents = sent_tokenize(seg_text)
        raw_sents = [clean_sentence(s) for s in raw_sents if len(s.strip()) > 3]

        # If only 1 sentence ‚Üí simple mapping
        if len(raw_sents) == 1:
            sentences.append(raw_sents[0])
            timestamps.append((seg_start, seg_end))
            continue

        # Precision weight using character length
        lengths = np.array([len(s) for s in raw_sents], dtype=float)
        proportions = lengths / lengths.sum()

        cursor = seg_start
        for s, frac in zip(raw_sents, proportions):
            dur = frac * (seg_end - seg_start)
            start = cursor
            end = cursor + dur
            sentences.append(s)
            timestamps.append((start, end))
            cursor = end

    from sklearn.feature_extraction.text import TfidfVectorizer

    # =====================================================================
    # 5Ô∏è‚É£ ADVANCED TF-IDF Candidate Selection (Density Normalized)
    # =====================================================================

    tfidf = TfidfVectorizer(
        stop_words="english",
        ngram_range=(1, 2),
        sublinear_tf=True,
        min_df=1,
    )

    X = tfidf.fit_transform(sentences)

    # Normalize short sentences (too short = too strong by default)
    tfidf_scores = np.asarray(X.sum(axis=1)).ravel()
    length_norm = np.array([max(30, len(s)) for s in sentences]) / 30
    tfidf_scores = tfidf_scores / length_norm

    # Position bias (start and middle weighted)
    pos_bias = np.linspace(1.1, 0.8, len(sentences))
    scores = tfidf_scores * pos_bias

    top_k = min(80, len(sentences))
    selected_idx = np.argsort(-scores)[:top_k]

    candidate_sents = [sentences[i] for i in selected_idx]
    candidate_times = [timestamps[i] for i in selected_idx]

    # =====================================================================
    # 6Ô∏è‚É£ STRUCTURE-AWARE Hierarchical Summarization
    # =====================================================================

    def smart_chunks(text, max_chars=1800):
        blocks, buff = [], []
        curr = 0

        for s in sent_tokenize(text):
            if curr + len(s) < max_chars:
                buff.append(s)
                curr += len(s)
            else:
                blocks.append(" ".join(buff))
                buff = [s]
                curr = len(s)

        if buff:
            blocks.append(" ".join(buff))

        return blocks


    text_block = " ".join(candidate_sents)
    chunks = smart_chunks(text_block)

    full_duration = float(ffmpeg.probe(preprocessed_path)["format"]["duration"])

    summaries = []

    for chunk in chunks:
        max_len = (
            220 if full_duration < 600 else
            280 if full_duration < 2400 else
            350
        )

        out = summarizer(
            chunk,
            max_length=max_len,
            min_length=50,
            temperature=0.05,
            do_sample=False
        )
        summaries.append(out[0]["summary_text"])

    # Second pass (hierarchical summarizer)
    summary_text = summarizer(
        " ".join(summaries),
        max_length=280,
        min_length=80,
        do_sample=False,
    )[0]["summary_text"]

    # Save summary
    with open(os.path.join(output_dir, "summary.txt"), "w") as f:
        f.write(summary_text)

    # =====================================================================
    # 7Ô∏è‚É£ ULTRA Semantic Segment Selection (Fast + Accurate)
    # =====================================================================

    def decide_target_duration(d):
        if d < 300: return d * 0.35
        if d < 900: return d * 0.28
        if d < 1800: return d * 0.20
        return d * 0.14

    target_duration = decide_target_duration(full_duration)

    # Keep everything on GPU
    emb_sents = embedding_model.encode(candidate_sents, batch_size=32, convert_to_tensor=True).to("cuda")
    emb_summary = embedding_model.encode(summary_text, convert_to_tensor=True).to("cuda")

    similarity_scores = util.cos_sim(emb_summary, emb_sents).cpu().numpy().flatten()
    ranked_idx = np.argsort(similarity_scores)[::-1]

    chosen_segments = []
    added_embeddings = []   # keep on GPU
    total = 0

    for idx in ranked_idx:
        seg_text = candidate_sents[idx]
        start, end = candidate_times[idx]
        dur = end - start

        if total + dur > target_duration:
            continue

        seg_emb = emb_sents[idx]  # stays on GPU

        # Redundancy elimination (GPU-based)
        redundant = False
        for e in added_embeddings:
            if util.cos_sim(seg_emb, e).item() >= 0.78:
                redundant = True
                break
        if redundant:
            continue

        # Add GPU embedding
        added_embeddings.append(seg_emb)

        # Add buffer for better cutting
        start = max(0, start - 0.35)
        end = min(full_duration, end + 0.35)

        chosen_segments.append((start, end, seg_text))
        total += dur

        if total >= target_duration:
            break

    # =====================================================================
    # Intro + Outro Guarantee
    # =====================================================================

    first_seg = result["segments"][0]
    last_seg = result["segments"][-1]

    if chosen_segments[0][0] > first_seg["start"]:
        chosen_segments.insert(
            0, (max(0, first_seg["start"] - 0.4), first_seg["end"] + 0.4, first_seg["text"])
        )

    if chosen_segments[-1][1] < last_seg["end"]:
        chosen_segments.append(
            (last_seg["start"] - 0.4, last_seg["end"] + 0.4, last_seg["text"])
        )

    chosen_segments = sorted(chosen_segments, key=lambda x: x[0])


    # =====================================================================
    # ü©π SMART MERGE (Context-aware)
    # =====================================================================

    def merge_segments(segs, threshold=0.35):
        merged = []

        for seg in sorted(segs, key=lambda x: x[0]):
            if not merged:
                merged.append(seg)
                continue

            p_start, p_end, p_txt = merged[-1]
            c_start, c_end, c_txt = seg

            # Merge if small gap OR segment text is continuation
            if (
                c_start <= p_end + threshold
                or c_txt.split(" ")[0].lower() in ["so", "then", "and", "next"]
            ):
                merged[-1] = (p_start, max(p_end, c_end), p_txt + " " + c_txt)
            else:
                merged.append(seg)

        return merged

    chosen_segments = merge_segments(chosen_segments)
    print("üìå Final highlight segments:", len(chosen_segments))

    # -------------------------------
    # 8Ô∏è‚É£ Create Highlight Clips
    # -------------------------------
    highlight_dir = os.path.join(output_dir, "highlights")
    os.makedirs(highlight_dir, exist_ok=True)
    clip_paths = []

    for i, (start, end, _) in enumerate(chosen_segments, 1):
        out_file = os.path.join(highlight_dir, f"clip_{i}.mp4")
        try:
            (
                ffmpeg.input(preprocessed_path, ss=start, to=end)
                .output(out_file, vcodec="libx264", acodec="aac", preset="fast")
                .overwrite_output()
                .run(quiet=True)
            )
            clip_paths.append(out_file)
        except ffmpeg.Error as e:
            print(f"‚ùå Error creating clip {i}: {e.stderr.decode()}")

    # -------------------------------
    # 9Ô∏è‚É£ Merge Clips Safely
    # -------------------------------
    # Re-encode clips to ensure uniform codec/frame rate
    tmp_clips = []
    for i, clip in enumerate(clip_paths):
        tmp_clip = os.path.join(highlight_dir, f"tmp_{i}.mp4")
        ffmpeg.input(clip).output(tmp_clip, vcodec='libx264', acodec='aac', r=30, preset='fast').overwrite_output().run(quiet=True)
        tmp_clips.append(tmp_clip)

    file_list_path = os.path.join(output_dir, "file_list.txt")
    with open(file_list_path, "w") as f:
        for clip in tmp_clips:
            f.write(f"file '{clip}'\n")

    final_video = os.path.join(output_dir, "summarized_video.mp4")
    ffmpeg.input(file_list_path, format='concat', safe=0).output(
        final_video, vcodec='libx264', acodec='aac', strict='experimental'
    ).overwrite_output().run(quiet=True)

    print("‚úÖ Final summarized video saved at:", final_video)
    return final_video